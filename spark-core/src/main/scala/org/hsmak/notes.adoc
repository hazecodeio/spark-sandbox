= Spark's Quick Guide
:icons: font

. download
. set env var
+
https://spark.apache.org/docs/latest/hadoop-provided.html[Using Spark's "Hadoop Free" Build]

 $ export SPARK_DIST_CLASSPATH=$(hadoop classpath)

. Run the first example to verify Spark installation

 $ ./bin/run-example SparkPi 10

. Next run the Scala-native spark shell

 $ ./bin/spark-shell

** We can specify the number of jobs to run:

 $ ./bin/spark-shell --master local[2]

** Python-based Spark shell:

 $ ./bin/pyspark --master local[2]

== Hadoop's Quick Guide

. Download
. Configure according to http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html#Pseudo-Distributed_Operation[Pseudo-Distributed Operation]
+
 $ vim etc/hadoop/core-site.xml
+
NOTE: Check Douglas 's tutorial on Safari. He added more for the namenode, datannode, and others
+
[source,xml]
----
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
</configuration>
----

. asdf



