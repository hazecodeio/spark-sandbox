== Hadoop's Quick Guide
:toc:
:toclevels: 3
:sectnums: 3
:sectnumlevels: 3
:icons: font

=== Installation

. Download https://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-3.1.1/hadoop-3.1.1.tar.gz[hadoop-3.1.1.tar.gz]
. Issue the following to extract the archive:

 $ mkdir -p /opt/tools/hadoop
 $ cd /opt/tools/hadoop
 $ tar -xvif hadoop-3.1.1/hadoop-3.1.1.tar.gz
 $ ln -s hadoop-3.1.1 hadoop-current

. Export the following environment variables:

 $ export HADOOP_HOME=/opt/tools/hadoop/hadoop-current
 $ export PATH=$PATH:$HADOOP_HOME/bin

=== Configurations

NOTE: Configurations in the following steps are according to http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html#Pseudo-Distributed_Operation[Pseudo-Distributed Operation],
but also somehow can be seen as a minimized http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/ClusterSetup.html[Cluster Setup]


. Modify the following files

 $ vim ./etc/hadoop/core-site.xml
+
So, it contains the following properties:
+
[source,xml]
----
include::etc/hadoop/core-site.xml[]
----



+
 $ vim ./etc/hadoop/hdfs-site.xml
+
So, it contains the following properties:
+
[source,xml]
----
include::etc/hadoop/hdfs-site.xml[]
----



+
 $ vim ./etc/hadoop/yarn-site.xml
+
So, it contains the following properties:
+
[source,xml]
----
include::etc/hadoop/yarn-site.xml[]
----



+
 $ vim ./etc/hadoop/mapred-site.xml
+
So, it contains the following properties:
+
[source,xml]
----
include::etc/hadoop/mapred-site.xml[]
----



. Create the following HDFS directories
+
 $ mkdir -p /var/data/hadoop/hdfs/nn
+
 $ mkdir -p /var/data/hadoop/hdfs/snn
+
 $ mkdir -p /var/data/hadoop/hdfs/dn

. Format the namenode
+
 $ hdfs namenode -format
+

. Start the HDFS Services:

 $ hdfs --daemon start namenode
 $ hdfs --daemon start secondarynamenode
 $ hdfs --daemon start datanode
+
You can check if all jobs are running in the background via:
+
 $ jps
+
NOTE: Replace 'start' with 'stop' in order to stop the same jobs.

. use 'hdfs' to create a job history server directory:

 $ hdfs dfs -mkdir -p /mr-history/tmp
 $ hdfs dfs -mkdir -p /mr-history/done


=== Verify Installation

* HDFS front page: http://localhost:9870/
* Yarn front page: http://localhost:8088/

=== Running Hadoop Application

. Start YARN job:

 $ yarn --daemon start resourcemanager
 $ yarn --daemon start nodemanager

. Start MapReduce job:

 $ mapred --daemon start historyserver

. Run a Hadoop example application

 $ yarn jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.1.jar pi 8 100000